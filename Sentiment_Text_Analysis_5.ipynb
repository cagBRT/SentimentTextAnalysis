{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 5.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyPqfZ3SfogqoUcV6F/hXA+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content/\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS"
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9FHBBM7FP0A"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf"
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ"
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN9x1Zgn_lN9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences_yelp = df_yelp['sentence'].values\n",
        "y_yelp = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_yelp, sentences_test_yelp, y_train_yelp, y_test_yelp = train_test_split(\n",
        "   sentences_yelp, y_yelp, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_yelp[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmO8T-mP_tOT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_amazon = df[df['source'] == 'amazon']\n",
        "\n",
        "sentences_amazon = df_amazon['sentence'].values\n",
        "y_amazon = df_amazon['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_amazon, sentences_test_amazon, y_train_amazon, y_test_amazon = train_test_split(\n",
        "   sentences_amazon, y_amazon, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_amazon[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer_yelp = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer_yelp.fit_on_texts(sentences_train_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUjzr7dhAwyy"
      },
      "source": [
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer_amazon = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer_amazon.fit_on_texts(sentences_train_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4deq14GHCC"
      },
      "source": [
        "The number assigned to each word is dependent upon is frequency of use in all the sentences. <br>\n",
        "For example:<br>\n",
        ">'the' is 1<br>\n",
        "'and' is 2<br>\n",
        "'was' is 3<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhtYbGiFWwT"
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train_yelp = tokenizer_yelp.texts_to_sequences(sentences_train_yelp)\n",
        "print(sentences_train_yelp[3],X_train_yelp[3])\n",
        "print(sentences_train_yelp[23],X_train_yelp[23])\n",
        "print(sentences_train_yelp[620],X_train_yelp[620])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNhHYRxSA43X"
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train_amazon = tokenizer_amazon.texts_to_sequences(sentences_train_amazon)\n",
        "print(sentences_train_amazon[3],X_train_amazon[3])\n",
        "print(sentences_train_amazon[23],X_train_amazon[23])\n",
        "print(sentences_train_amazon[620],X_train_amazon[620])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT22An4iFduH"
      },
      "source": [
        "X_test_yelp = tokenizer_yelp.texts_to_sequences(sentences_test_yelp)\n",
        "vocab_size_yelp = len(tokenizer_yelp.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIYm7EiPBHAt"
      },
      "source": [
        "X_test_amazon= tokenizer_amazon.texts_to_sequences(sentences_test_amazon)\n",
        "vocab_size_amazon = len(tokenizer_amazon.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBx3rLC0IQb"
      },
      "source": [
        "The indexing begins with the most common word first (the). <br>\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkVfwDTzm0a"
      },
      "source": [
        "for word in ['the', 'all', 'bad', 'terrible','horrible','lost','lukewarm','bacon']: \n",
        "    print('{}: {}'.format(word, tokenizer_yelp.word_index[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5hojTdHTAq"
      },
      "source": [
        "The list can be searched by word or by index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLJRwI9G2CS"
      },
      "source": [
        "#What is the least used word in the list? \n",
        "print((tokenizer_yelp.index_word[1746]))\n",
        "print((tokenizer_amazon.index_word[1573]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPB6nRbJkUgQ"
      },
      "source": [
        "# **Pad the sequence of words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZnB5SD1E82"
      },
      "source": [
        "One problem that we have is that each text sequence has different number of words. To fix this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmvBgUk1yRg"
      },
      "source": [
        "The resulting feature vector contains mostly zeros, when you have a fairly short sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FkOsNL1Jl8"
      },
      "source": [
        "from keras.utils import pad_sequences\n",
        "#The maximum length of a review, cut off the extra words \n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train_yelp = pad_sequences(X_train_yelp, padding='post', maxlen=maxlen)\n",
        "X_test_yelp = pad_sequences(X_test_yelp, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train_yelp.shape,X_test_yelp.shape)\n",
        "print(y_train_yelp.shape,y_test_yelp.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(\"The review:\\n\",sentences_train_yelp[index])\n",
        "print(\"\\nThe final feature vector:\\n\",X_train_yelp[index, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doeObkBdIJ5Z"
      },
      "source": [
        "#The maximum length of a review, cut off the extra words \n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train_amazon = pad_sequences(X_train_amazon, padding='post', maxlen=maxlen)\n",
        "X_test_amazon = pad_sequences(X_test_amazon, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train_amazon.shape,X_test_amazon.shape)\n",
        "print(y_train_amazon.shape,y_test_amazon.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(\"The review:\\n\",sentences_train_amazon[index])\n",
        "print(\"\\nThe final feature vector:\\n\",X_train_amazon[index, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4ZMILvfWr9"
      },
      "source": [
        "# **Train the Embedded Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImkmiOQ1632"
      },
      "source": [
        "Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. <br>\n",
        "You will need the following parameters:<br>\n",
        "\n",
        ">input_dim: the size of the vocabulary<br>\n",
        "output_dim: the size of the dense vector<br>\n",
        "input_length: the length of the sequence<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8ud39NJsAn"
      },
      "source": [
        "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
        "\n",
        "To connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_A0dS0p17hu"
      },
      "source": [
        "embedding_dim = 50\n",
        "input_dim_yelp=vocab_size_yelp\n",
        "\n",
        "model_yelp = Sequential()\n",
        "model_yelp.add(layers.Embedding(input_dim_yelp, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model_yelp.add(layers.Flatten())\n",
        "model_yelp.add(layers.Dense(10, activation='relu'))\n",
        "model_yelp.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_yelp.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"input dim=\",input_dim_yelp)\n",
        "print(\"output dim of embedding layer=\",embedding_dim)\n",
        "print(\"input length = \", maxlen)\n",
        "model_yelp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvhLLPFhHMU4"
      },
      "source": [
        "embedding_dim = 30\n",
        "input_dim_amazon=vocab_size_amazon\n",
        "\n",
        "model_amazon = Sequential()\n",
        "model_amazon.add(layers.Embedding(input_dim_amazon, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model_amazon.add(layers.Flatten())\n",
        "model_amazon.add(layers.Dense(10, activation='relu'))\n",
        "model_amazon.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_amazon.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"input dim=\",input_dim_amazon)\n",
        "print(\"output dim of embedding layer=\",embedding_dim)\n",
        "print(\"input length = \", maxlen)\n",
        "model_amazon.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuZ3mihe3I1l"
      },
      "source": [
        "print(X_train_yelp.shape,X_test_yelp.shape)\n",
        "print(y_train_yelp.shape,y_test_yelp.shape)\n",
        "\n",
        "history_yelp = model_yelp.fit(X_train_yelp, y_train_yelp,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_yelp,y_test_yelp),\n",
        "                    batch_size=10)\n",
        "\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_train_yelp, y_train_yelp, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy_yelp))\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_test_yelp, y_test_yelp, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_yelp))\n",
        "plot_history(history_yelp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWneLk-THa3K"
      },
      "source": [
        "print(X_train_amazon.shape,X_test_amazon.shape)\n",
        "print(y_train_amazon.shape,y_test_amazon.shape)\n",
        "\n",
        "history_amazon = model_amazon.fit(X_train_amazon, y_train_amazon,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_amazon,y_test_amazon),\n",
        "                    batch_size=10)\n",
        "\n",
        "loss_amazon, accuracy_amazon = model_amazon.evaluate(X_train_amazon, y_train_amazon, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy_amazon))\n",
        "loss_amazon, accuracy_amazon = model_amazon.evaluate(X_test_amazon, y_test_amazon, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_amazon))\n",
        "plot_history(history_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdnxohmj4Bsy"
      },
      "source": [
        "This is typically a not very reliable way to work with sequential data as you can see in the performance. When working with sequential data you want to focus on methods that look at local and sequential information instead of absolute positional information."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yvS9-jWQlPiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh_f1H7AfoXI"
      },
      "source": [
        "# **Use a MaxPooling Layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxV9Z7lU4KDb"
      },
      "source": [
        "Another way to work with embeddings is by using a MaxPooling1D/AveragePooling1D or a GlobalMaxPooling1D/GlobalAveragePooling1D layer after the embedding. You can think of the pooling layers as a way to downsample (a way to reduce the size of) the incoming feature vectors.\n",
        "\n",
        "In the case of max pooling you take the maximum value of all features in the pool for each feature dimension. In the case of average pooling you take the average, but max pooling seems to be more commonly used as it highlights large values.\n",
        "\n",
        "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIkoyLv-fqRf"
      },
      "source": [
        "Global max pooling = ordinary max pooling layer with pool size equals to the size of the input.<br>\n",
        "\n",
        "Advantages of Global Pooling:\n",
        "* it is more native to the convolution structure by enforcing correspondences between feature maps and categories.\n",
        "* there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcohC3N4NCn"
      },
      "source": [
        "embedding_dim = 50\n",
        "input_dim=vocab_size_yelp\n",
        "\n",
        "model_yelp = Sequential()\n",
        "model_yelp.add(layers.Embedding(input_dim, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model_yelp.add(layers.GlobalMaxPool1D())\n",
        "model_yelp.add(layers.Dense(10, activation='relu'))\n",
        "model_yelp.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_yelp.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_yelp.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MU5Xh1l4Tjo"
      },
      "source": [
        "history_yelp = model_yelp.fit(X_train_yelp, y_train_yelp,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_yelp, y_test_yelp),\n",
        "                    batch_size=10)\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_train_yelp, y_train_yelp, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy_yelp))\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_test_yelp, y_test_yelp, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_yelp))\n",
        "plot_history(history_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_Y_6-AgLMj"
      },
      "source": [
        "# **Assignment #11:** \n",
        "Use the Amazon dataset to train the model with a max pooling layer. "
      ]
    }
  ]
}