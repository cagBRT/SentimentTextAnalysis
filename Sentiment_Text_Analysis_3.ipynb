{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyMolHW+PDfVMAxPt5pojbKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content/\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS"
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf"
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ"
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15"
      },
      "source": [
        "# **Split the review data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences_yelp = df_yelp['sentence'].values\n",
        "y_yelp = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator;\n",
        "#If RandomState instance, random_state is the random number generator;\n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_yelp, sentences_test_yelp, y_train_yelp, y_test_yelp = train_test_split(\n",
        "   sentences_yelp, y_yelp, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_yelp[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-sq8c-Iegs9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_amazon = df[df['source'] == 'amazon']\n",
        "\n",
        "sentences_amazon = df_amazon['sentence'].values\n",
        "y_amazon = df_amazon['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator;\n",
        "#If RandomState instance, random_state is the random number generator;\n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_amazon, sentences_test_amazon, y_train_amazon, y_test_amazon = train_test_split(\n",
        "   sentences_amazon, y_amazon, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_amazon[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6ZEW6238GM"
      },
      "source": [
        "# **Word Embedding**\n",
        "There are various ways to vectorize text, such as:\n",
        "*   Words represented as a vector.\n",
        "*   Characters represented as a vector\n",
        "\n",
        "\n",
        "In this notebook, youâ€™ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are:\n",
        "*   one-hot encoding\n",
        "*   word embeddings<br>\n",
        "\n",
        "The first example shown below does one-hot encoding. The second example does word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocESp_nW4Nff"
      },
      "source": [
        "**Hot-one encoding Example**<br>\n",
        "In this example you experiment with one hot encoding on a small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmlZhoC3-X5"
      },
      "source": [
        "#There are five elements, vocabulary size of three\n",
        "food = ['bacon', 'egg', 'bacon', 'beer', 'egg']\n",
        "food"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bKNLRvaAXqH"
      },
      "source": [
        "Convert the foods into one their one-hot equivalents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avkg8U2t4VWT"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "food_labels = encoder.fit_transform(food)\n",
        "\n",
        "#create a dataframe to examine the data\n",
        "df = pd.DataFrame()\n",
        "df['food']= food\n",
        "df['food_labels']= food_labels\n",
        "df.sort_values(by=['food'])\n",
        "\n",
        "#Convert the food into one-hot values\n",
        "cat_columns = [\"food_labels\"]\n",
        "df_processed = pd.get_dummies(df, prefix_sep=\"__\",columns=cat_columns)\n",
        "df_processed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIzHxBvk4mez"
      },
      "source": [
        "#Or use the OneHotEncoder method to encode the data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "food_labels = food_labels.reshape((5, 1))\n",
        "encoder.fit_transform(food_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrlu8m1jQVyw"
      },
      "source": [
        "# **Assignment #6:**\n",
        "1. Create a list of 10 household appliances.\n",
        "2. Convert the list into one-hot encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZe8bcPNRNlz"
      },
      "source": [
        "# **Discussion:**\n",
        "What are some downsides of one-hot encoding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYyfbnJjPS_"
      },
      "source": [
        "# **Word embedding**<br>\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ExQx2fbmh8"
      },
      "source": [
        "page(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE096qUiyYPM"
      },
      "source": [
        "Word embedding represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
        "\n",
        "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.<br>\n",
        "\n",
        "This would map semantically similar words close on the embedding space like numbers or colors. If the embedding captures the relationship between words well, things like vector arithmetic should become possible. A famous example in this field of study is the ability to map King - Man + Woman = Queen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0c2w2KtRnij"
      },
      "source": [
        "Word embedding has fewer dimensions than one-hot encoding<br>\n",
        "Word embedding places similar words near each other<br>\n",
        "One-hot encoding has a sparse matrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EAi8ZEyjnH"
      },
      "source": [
        "There are two methods for doing word embedding: <br>\n",
        "\n",
        ">1.Train your word embeddings during the training of your neural network. <br>\n",
        ">2.Use pretrained word embeddings which you can directly use in your model. You can leave these word embeddings unchanged during training or you can train them.<br><br>\n",
        "\n",
        "Then tokenize the data into a format that can be used by the word embeddings. <br><br>\n",
        "Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.<br>\n",
        "\n",
        "[Keras Tokenizer ](https://keras.io/preprocessing/text/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer.fit_on_texts(sentences_train_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4deq14GHCC"
      },
      "source": [
        "The number assigned to each word is dependent upon is frequency of use in all the sentences. <br>\n",
        "For example:<br>\n",
        ">'the' is 1<br>\n",
        "'and' is 2<br>\n",
        "'was' is 3<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhtYbGiFWwT"
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train_yelp = tokenizer.texts_to_sequences(sentences_train_yelp)\n",
        "print(sentences_train_yelp[3],X_train_yelp[3])\n",
        "print(sentences_train_yelp[23],X_train_yelp[23])\n",
        "print(sentences_train_yelp[620],X_train_yelp[620])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT22An4iFduH"
      },
      "source": [
        "X_test_yelp = tokenizer.texts_to_sequences(sentences_test_yelp)\n",
        "vocab_size_yelp = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBx3rLC0IQb"
      },
      "source": [
        "The indexing begins with the most common word first (the). <br>\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkVfwDTzm0a"
      },
      "source": [
        "for word in ['the', 'all', 'bad', 'terrible','horrible','lost','lukewarm','bacon','atom']:\n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5hojTdHTAq"
      },
      "source": [
        "The list can be searched by word or by index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLJRwI9G2CS"
      },
      "source": [
        "#What is the least used word in the list?\n",
        "print((tokenizer.index_word[1746]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6F_ZYOCUUIZ"
      },
      "source": [
        "# **Assignment #7:**\n",
        "Use the Amazon reviews to do word embedding.\n",
        "<br>\n",
        "Use different variable names than the ones used for the Yelp reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJBNUDVtmlH"
      },
      "source": [
        "# **Find similar words with gensim**<br>\n",
        "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne2MZqlUbjzo"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "#Find words similiar to other words\n",
        "#If done correctly, you can do math with words\n",
        "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))\n",
        "#Try sandwich, tuna, bread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2jxdrUeU6Ap"
      },
      "source": [
        "# **Assignment #8:**\n",
        "Use the gensim library to find other word equations. <br>\n",
        "Share them with the class."
      ]
    }
  ]
}