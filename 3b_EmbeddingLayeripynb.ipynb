{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7ubahhxfX1pJoqJD3DNGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/3b_EmbeddingLayeripynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layers are an essential part of neural networks.\n",
        "In this notebook we will look at a simple example of an embedded layer."
      ],
      "metadata": {
        "id": "i4kSK27COwXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvYqUGCvNztp"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layers are mainlyused in NLP.\n",
        "With Embedding layers you can use pre-trained embeddings (like GloVe) or train your own."
      ],
      "metadata": {
        "id": "gDWftfERPDoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three parameters to the embedding layer<br>\n",
        "\n",
        "input_dim : Size of the vocabulary<br>\n",
        "output_dim : Length of the vector for each word<br>\n",
        "input_length : Maximum length of a sequence<br>"
      ],
      "metadata": {
        "id": "qgHtz51BQDxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile('adam','mse')"
      ],
      "metadata": {
        "id": "Nvzcf42bN2xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions."
      ],
      "metadata": {
        "id": "oPvLZEvOPg8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing this method allows us to thig of the embedding layer like a lookup table. The words are the keys in this table, while the dense word vectors are the values"
      ],
      "metadata": {
        "id": "VRtzwThePnie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you will see in the output below, each word (1 and 2) is represented by a vector of length 4"
      ],
      "metadata": {
        "id": "p2K6KoxzOAjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = np.array([[1,2]])\n",
        "pred = model.predict(input_data)\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "pE1BdrLQN5oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lUmF5-6rQ36G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example with a small dataset"
      ],
      "metadata": {
        "id": "sBs8XGOdQ5DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Embedding,Dense"
      ],
      "metadata": {
        "id": "fKAJY9PjOMH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this small dataset we have 10 reviews, 5 postive and 5 negative\n"
      ],
      "metadata": {
        "id": "iAMyV94sQ_GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 restaurant reviews\n",
        "reviews =[\n",
        "          'Never coming back!',\n",
        "          'horrible service',\n",
        "          'rude waitress',\n",
        "          'cold food',\n",
        "          'horrible food!',\n",
        "          'awesome',\n",
        "          'awesome services!',\n",
        "          'rocks',\n",
        "          'poor work',\n",
        "          'couldn\\'t have done better'\n",
        "]\n",
        "#Define labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "amf-UKlHOTDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see the length of each encoded review is equal to the number of words in that review. Keras one_hot is basically converting each word into its one-hot encoded index. Now we need to apply padding so that all the encoded reviews are of same length. Let’s define 4 as the maximum length and pad the encoded vectors with 0’s in the end."
      ],
      "metadata": {
        "id": "zomPDEl7ObWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vocab_size = 50\n",
        "encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]\n",
        "print(f'encoded reviews: {encoded_reviews}')"
      ],
      "metadata": {
        "id": "mMwSEUp_OVWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see the length of each encoded review is equal to the number of words in that review. Keras one_hot is basically converting each word into its one-hot encoded index. Now we need to apply padding so that all the encoded reviews are of same length. Let’s define 4 as the maximum length and pad the encoded vectors with 0’s in the end."
      ],
      "metadata": {
        "id": "tDRyqKifRP2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "id": "OzVWFAy-Odec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "U34k6xc9Rxvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "HHEdbVJUOgfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "VhKxlVRCR1FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training is completed, embedding layer has learnt the weights which are nothing but the vector representations of each word. Lets check the shape of the weight matrix"
      ],
      "metadata": {
        "id": "Ed2G3lOUR8Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_reviews,labels,epochs=100,verbose=0)"
      ],
      "metadata": {
        "id": "sA18wYcJOj-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This embedding matrix is essentially a lookup table of 50 rows and 8 columns, as evident by the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "no36R9VcR-_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.get_weights()[0].shape)"
      ],
      "metadata": {
        "id": "5mp2UyztOmGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.embeddings)"
      ],
      "metadata": {
        "id": "nfsZvIKKSBl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}