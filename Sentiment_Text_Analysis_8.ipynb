{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 8.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyOCbzQmU1a5cZcwvZ4vCuUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUmmVaSCDTV"
      },
      "source": [
        "# **Mount your Google Drive on this CoLab Notebook**\n",
        "Execute the following code cell<br>\n",
        "Click on the given link<br>\n",
        "Select your user name<br>\n",
        "Click **Allow**<br>\n",
        "Copy the authorization code<br>\n",
        "Paste the authorization code into the user input box. <br>\n",
        "You Google Drive is mounted to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0Y2vgPCDfW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content/\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_h3O7HD08K"
      },
      "source": [
        "# **Check that your drive is mounted**\n",
        "1. On the menu bar, click the **folder icon**<br>\n",
        "2. Click on the **folder icon with the up arrow**\n",
        "3. Click on **gdrive**\n",
        "4. Click on **My Drive**\n",
        "5. Check for the file called **wiki-news-300d-1M.vec **<br>\n",
        "If the file is there, you have correctly installed the necessary files for this notebook. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS"
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9FHBBM7FP0A"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf"
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ"
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN9x1Zgn_lN9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences_yelp = df_yelp['sentence'].values\n",
        "y_yelp = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator;\n",
        "#If RandomState instance, random_state is the random number generator;\n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_yelp, sentences_test_yelp, y_train_yelp, y_test_yelp = train_test_split(\n",
        "   sentences_yelp, y_yelp, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_yelp[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmO8T-mP_tOT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_amazon = df[df['source'] == 'amazon']\n",
        "\n",
        "sentences_amazon = df_amazon['sentence'].values\n",
        "y_amazon = df_amazon['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator;\n",
        "#If RandomState instance, random_state is the random number generator;\n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_amazon, sentences_test_amazon, y_train_amazon, y_test_amazon = train_test_split(\n",
        "   sentences_amazon, y_amazon, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_amazon[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer_yelp = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer_yelp.fit_on_texts(sentences_train_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUjzr7dhAwyy"
      },
      "source": [
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer_amazon = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer_amazon.fit_on_texts(sentences_train_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4deq14GHCC"
      },
      "source": [
        "The number assigned to each word is dependent upon is frequency of use in all the sentences. <br>\n",
        "For example:<br>\n",
        ">'the' is 1<br>\n",
        "'and' is 2<br>\n",
        "'was' is 3<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhtYbGiFWwT"
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train_yelp = tokenizer_yelp.texts_to_sequences(sentences_train_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNhHYRxSA43X"
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train_amazon = tokenizer_amazon.texts_to_sequences(sentences_train_amazon)\n",
        "print(sentences_train_amazon[3],X_train_amazon[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT22An4iFduH"
      },
      "source": [
        "X_test_yelp = tokenizer_yelp.texts_to_sequences(sentences_test_yelp)\n",
        "vocab_size_yelp = len(tokenizer_yelp.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIYm7EiPBHAt"
      },
      "source": [
        "X_test_amazon= tokenizer_amazon.texts_to_sequences(sentences_test_amazon)\n",
        "vocab_size_amazon = len(tokenizer_amazon.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPB6nRbJkUgQ"
      },
      "source": [
        "# **Pad the sequence of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FkOsNL1Jl8"
      },
      "source": [
        "from keras.utils import pad_sequences\n",
        "#The maximum length of a review, cut off the extra words\n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train_yelp = pad_sequences(X_train_yelp, padding='post', maxlen=maxlen)\n",
        "X_test_yelp = pad_sequences(X_test_yelp, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train_yelp.shape,X_test_yelp.shape)\n",
        "print(y_train_yelp.shape,y_test_yelp.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(\"The review:\\n\",sentences_train_yelp[index])\n",
        "print(\"\\nThe final feature vector:\\n\",X_train_yelp[index, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doeObkBdIJ5Z"
      },
      "source": [
        "#The maximum length of a review, cut off the extra words\n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train_amazon = pad_sequences(X_train_amazon, padding='post', maxlen=maxlen)\n",
        "X_test_amazon = pad_sequences(X_test_amazon, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train_amazon.shape,X_test_amazon.shape)\n",
        "print(y_train_amazon.shape,y_test_amazon.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(\"The review:\\n\",sentences_train_amazon[index])\n",
        "print(\"\\nThe final feature vector:\\n\",X_train_amazon[index, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkM6CvrglKtG"
      },
      "source": [
        "# **Use a precomputed embedding space**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1S2XEG15FNr"
      },
      "source": [
        "Can performance be improved using a precomputed embedding space that utilizes a much larger corpus? <br>\n",
        "It is possible to precompute word embeddings by simply training them on a large corpus of text. Among the most popular methods are Word2Vec developed by Google and GloVe (Global Vectors for Word Representation) developed by the Stanford NLP Group.<br>\n",
        "\n",
        "Word2Vec achieves this by employing neural networks and GloVe achieves this with a co-occurrence matrix and by using matrix factorization. <br>\n",
        "In both cases you are dealing with dimensionality reduction:  <br>\n",
        ">Word2Vec is more accurate  <br>\n",
        "GloVe is faster to compute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHM_LB98qW3"
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yg52fWY8vrf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix_yelp = create_embedding_matrix(\n",
        "    '/gdrive/My Drive/wiki-news-300d-1M.vec',\n",
        "    tokenizer_yelp.word_index, embedding_dim)\n",
        "print(embedding_matrix_yelp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ3OMPR7Sdl6"
      },
      "source": [
        "embedding_matrix_amazon = create_embedding_matrix(\n",
        "    '/gdrive/My Drive/wiki-news-300d-1M.vec',\n",
        "    tokenizer_amazon.word_index, embedding_dim)\n",
        "print(embedding_matrix_amazon.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrIyzaHZmHC"
      },
      "source": [
        "Percentage of vocabulary covered by the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zerTRtUWZhbt"
      },
      "source": [
        "nonzero_elements_yelp = np.count_nonzero(np.count_nonzero(embedding_matrix_yelp, axis=1))\n",
        "nonzero_elements_yelp / vocab_size_yelp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chp0_eDHR0MW"
      },
      "source": [
        "nonzero_elements_amazon = np.count_nonzero(np.count_nonzero(embedding_matrix_amazon, axis=1))\n",
        "nonzero_elements_amazon / vocab_size_amazon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrmOUG3jSB6k"
      },
      "source": [
        "# **Convolututional Neural Network (CNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3MaMvPacWb"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model_yelp = Sequential()\n",
        "model_yelp.add(layers.Embedding(vocab_size_yelp, embedding_dim, input_length=maxlen))\n",
        "model_yelp.add(layers.Conv1D(128, 5, activation='relu',name=\"c1\"))\n",
        "model_yelp.add(layers.GlobalMaxPooling1D())\n",
        "model_yelp.add(layers.Dense(10, activation='relu'))\n",
        "model_yelp.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_yelp.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_yelp.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyCPr710hxYE"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model_amazon = Sequential()\n",
        "model_amazon.add(layers.Embedding(vocab_size_yelp, embedding_dim, input_length=maxlen))\n",
        "model_amazon.add(layers.Conv1D(128, 5, activation='relu',name=\"c1\"))\n",
        "model_amazon.add(layers.GlobalMaxPooling1D())\n",
        "model_amazon.add(layers.Dense(10, activation='relu'))\n",
        "model_amazon.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_amazon.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_amazon.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPXIiu8agJH"
      },
      "source": [
        "history_yelp = model_yelp.fit(X_train_yelp, y_train_yelp,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_yelp, y_test_yelp),\n",
        "                    batch_size=10)\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_train_yelp, y_train_yelp, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy_yelp))\n",
        "loss_yelp, accuracy_yelp = model_yelp.evaluate(X_test_yelp, y_test_yelp, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_yelp))\n",
        "plot_history(history_yelp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GkmxPwDiEBF"
      },
      "source": [
        "history_amazon = model_amazon.fit(X_train_amazon, y_train_amazon,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_amazon, y_test_amazon),\n",
        "                    batch_size=10)\n",
        "loss_amazon, accuracy_amazon = model_amazon.evaluate(X_train_amazon, y_train_amazon, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy_amazon))\n",
        "loss_amazon, accuracy_amazon = model_amazon.evaluate(X_test_amazon, y_test_amazon, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_amazon))\n",
        "plot_history(history_amazon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR2yiXOvTL0v"
      },
      "source": [
        "# **HyperParameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os0yrNZaapZ_"
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEXBJP4ktau"
      },
      "source": [
        "**Embedding dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLd-_2jQasgh"
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000],\n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dW84sk_fdT2"
      },
      "source": [
        "# **HyperParameter Grid Search of each text set**\n",
        "Perform the random search method of hyperparameter tuning to improve the model performance. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiYSjh57NlnG"
      },
      "source": [
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_full_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df_full = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df_full['source'] = source  # Add another column filled with the source name\n",
        "    df_full_list.append(df)\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncCjPwCkmsYY"
      },
      "source": [
        "# **This will take approximately 20 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "k5GnDF7guzsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkYfUuKaww4"
      },
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/gdrive/My Drive/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False, vocab_size=4603)\n",
        "\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/CNN Results.png\" , width=600)"
      ],
      "metadata": {
        "id": "lbixj8as3k48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfvbZPSoTU3j"
      },
      "source": [
        "# **HyperParameter Tuning on all the datasets together**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ShY0OgEZqPJ"
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_full_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df_full = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df_full['source'] = source  # Add another column filled with the source name\n",
        "    df_full_list.append(df)\n",
        "\n",
        "df_full = pd.concat(df_list)\n",
        "print(df_full.iloc[2000])\n",
        "print(\"dataframe shape: \",df_full.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm86EImSbLBh"
      },
      "source": [
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/gdrive/My Drive/output.txt'\n",
        "\n",
        "print('Running grid search for data set :\\n', df_full)\n",
        "sentences = df_full['sentence'].values\n",
        "y = df_full['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "    sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "# Save and evaluate results\n",
        "with open(output_file, 'a') as f:\n",
        "   s = ('Running {} data set\\nBest Accuracy : '\n",
        "        '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "   output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "   print(output_string)\n",
        "   f.write(output_string)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}