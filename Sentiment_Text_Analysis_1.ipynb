{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyOwOQs8U52RTqKDBsFwKmVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content/\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43caXH-JktTN",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-requisites:**<br>\n",
        "Python - 2 day course is sufficient<br>\n",
        "Keras understanding - intro course is sufficient<br>\n",
        "logistic regression - understanding<br>\n",
        "Deep neural networks - intro course is sufficient<br>\n",
        "Hyperparameter tuning _ intro course is sufficient<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP-R61seyu6s",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-work**\n",
        "**Do this step before coming to class. It may take as long as 30 minutes to complete the download and upload.**<br>\n",
        "<br>\n",
        "You will need to add a file to your google drive. <br>\n",
        "1. Download the file to your computer. \n",
        ">Click on the link below. Then click **Download** <br>\n",
        "The download can take as long as 15 minutes.<br>\n",
        "The file to download is: [fileToAddToGoogleDrive](https://drive.google.com/open?id=1zJI1Xz-CgaQqX1UtBcOhUjEKWcSt6QK6)<br>\n",
        "The file is large: 2GBytes<br><br>\n",
        "\n",
        "\n",
        "2. Upload the file to Google Drive:<br>\n",
        ">Open Google Drive<br>\n",
        "On the Drive menu, click on **New** >> **File Upload**<br>\n",
        "Find the file on your computer, click on it and upload the file. \n",
        "\n",
        "The file is large, it may take as long as 15 minutes<br>\n",
        "Once the file is on you<br><br>\n",
        "The file is from a website: [English word vectors](https://fasttext.cc/docs/en/english-vectors.html)<br>\n",
        "This page gathers several pre-trained word vectors trained using fastText."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a68Nq4HgJDF",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Text Analyis**<br>\n",
        "This notebook introduces basic machine learning methods for determing if yelp, Amazon, and imdb reviews are positive or negative. <br>\n",
        "The following topics are covered in this notebook: \n",
        "1. Create and train a baseline model.\n",
        "2. Create and train a simple DNN model\n",
        "3. Add an embedded layer to a simple DNN model\n",
        "4. Add a MaxPooling layer to an embedded model\n",
        "5. Use a pretrained embedded model\n",
        "6. Create a CNN model with an embedded layer\n",
        "7. Do hyperparameter tuning using random search\n",
        "\n",
        "It does not cover recurrent neural networks or LSTM or GRU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYY6S0GBIYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfsq6tt92qyj",
        "colab_type": "text"
      },
      "source": [
        "**What is sentiment analysis?**<br>\n",
        "Sentiment analysis is a branch of natural language processing. It is one of the most important sources in decision making for determining the sentiment of language. It can extract from, identify, evaluate, or characterize online sentiment of reviews.<br>\n",
        "\n",
        "Sentiment analysis assumes various forms, from models that focus on polarity (positive, negative, neutral) to those that detect feelings and emotions (angry, happy, sad, etc), or even models that identify intentions (e.g. interested v. not interested).<br><br>\n",
        "\n",
        "It’s estimated that 80% of the world’s data is unstructured, in other words it’s unorganized. Huge amounts of text data (emails, support tickets, chats, social media conversations, surveys, articles, documents, etc), is created every day but it’s hard to analyze, understand, and sort through, not to mention time-consuming and expensive.\n",
        "\n",
        "Sentiment analysis, however, helps businesses make sense of all this unstructured text by automatically tagging it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpr1ZIVa47D_",
        "colab_type": "text"
      },
      "source": [
        "**Benefits of sentiment analysis**<br>\n",
        "Benefits of sentiment analysis include:\n",
        "\n",
        "1. Processes data at scale: Sentiment analysis helps businesses process huge amounts of data in an efficient and cost-effective way.\n",
        "\n",
        "2. Real-Time analysis: Sentiment analysis can identify critical issues in real-time, for example is a PR crisis on social media escalating? Is an angry customer about to churn? Sentiment analysis models can  immediately identify, through text, these types of situations. \n",
        "\n",
        "3. Consistent criteria: Tagging text by sentiment is highly subjective, influenced by personal experiences, thoughts, and beliefs. It’s estimated that people only agree around 60-65% of the time when determining the sentiment of a particular text. By using a centralized sentiment analysis system, companies can apply the same criteria to all of their data, helping them improve accuracy and gain better insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUmmVaSCDTV",
        "colab_type": "text"
      },
      "source": [
        "# **Mount your Google Drive on this CoLab Notebook**\n",
        "Execute the following code cell<br>\n",
        "Click on the given link<br>\n",
        "Select your user name<br>\n",
        "Click **Allow**<br>\n",
        "Copy the authorization code<br>\n",
        "Paste the authorization code into the user input box. <br>\n",
        "You Google Drive is mounted to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0Y2vgPCDfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_h3O7HD08K",
        "colab_type": "text"
      },
      "source": [
        "# **Check that your drive is mounted**\n",
        "1. On the menu bar, click the **folder icon**<br>\n",
        "2. Click on the **folder icon with the up arrow**\n",
        "3. Click on **gdrive**\n",
        "4. Click on **My Drive**\n",
        "5. Check for the file called **wiki-news-300d-1M.vec **<br>\n",
        "If the file is there, you have correctly installed the necessary files for this notebook. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fFT8ZFeTDT",
        "colab_type": "text"
      },
      "source": [
        "# **scikit-learn**\n",
        "This notebook uses the [scikit-learn](https://scikit-learn.org/stable/) library<br>\n",
        "Scikit-learn is a free software machine learning library for Python.<br>\n",
        "*   Simple and efficient tools for predictive data analysis\n",
        "*  Accessible to everybody, and reusable in various contexts\n",
        "*Built on NumPy, SciPy, and matplotlib\n",
        "*Open source, commercially usable - BSD license\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3",
        "colab_type": "text"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg",
        "colab_type": "text"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrPFs_FfJOB",
        "colab_type": "text"
      },
      "source": [
        "# **Create a Bag of Words**\n",
        "The bag-of-words model is a word/sentence representation used in NLP and information retrieval. <br>\n",
        "In this model, text is represented as a bag of its words, *disregarding grammar and even word order*.\n",
        "\n",
        "Create a bag of words (BoW) for vectorizing the text. <br>\n",
        "Take the data and create a vocabulary from all the words in all reviews.<br>\n",
        "The collection of texts is also called a **corpus** in NLP.<br>\n",
        "The **vocabulary** in this case is a list of words that occurred in our text where each word has its own index.<br>\n",
        "The resulting vector is also called a **feature vector**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM4CfF-1O7fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzPfHZZhHJS",
        "colab_type": "text"
      },
      "source": [
        "# **Bag of Words Example:** <br>\n",
        "# **Create a bag of words (BoW) for vectorizing the text**\n",
        "\n",
        "Create a toy set of words to demonstrate BoW.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VqhzIZd_hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1. The words\n",
        "john_words = ['Sam likes to run.', \n",
        "              'John hates to be cold and he hates to run.', \n",
        "              'John hates to be late.'\n",
        "            ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UPXWFMeFTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Create the bag of words\n",
        "#The words are in alphabetical order (uppercase listed before lowercase)\n",
        "#Each word is assigned a number\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(john_words)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-AeTLVewzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3. Convert the sentences in to vectors\n",
        "#each word used in a sentence has a '1' in the corresponding column\n",
        "dfbow = pd.DataFrame()\n",
        "dfbow['sentences'] = john_words\n",
        "vector = vectorizer.transform(john_words).toarray().tolist()\n",
        "dfbow['vector'] = vector\n",
        "print(vocab)\n",
        "dfbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_obQxnIey13Q",
        "colab_type": "text"
      },
      "source": [
        "Notice in the above output: \n",
        "1. All the words in all the sentences created the vocabulary.<br>\n",
        "2. Each sentence is turned into a vector. <br>\n",
        "3. The length of the vector is the length of the vocabulary.<br>\n",
        "4. A '1' in the vector indicates the word is used in the sentence, a '0' means it is not used in the sentence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vHNtHmOL0lQ",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 1**: <br>\n",
        "1. Write 5 sentences. \n",
        "2. Create a BoW from the sentences. \n",
        "3. Vectorize the sentences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeh-7rsK2chJ",
        "colab_type": "text"
      },
      "source": [
        "Although Bag-Of-Words model is the most widely used technique for sentiment analysis, it has two major weaknesses: <br>\n",
        "* the larger the vocabulary, the larger the feature vector\n",
        "* it assumes each word is independent of other words\n",
        "* the feature vectors are extremely sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0m107q4SwNq",
        "colab_type": "text"
      },
      "source": [
        "A **sparse matrix**  is a matrix in which most of the elements are zero<br>\n",
        "A **dense matrix**  is a matrix in which most of the elements are nonzero\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15",
        "colab_type": "text"
      },
      "source": [
        "# **Split the review data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0D7qx5gPtD",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorize the training and test set**\n",
        "Vectorize the data: <br>\n",
        "\n",
        "Assign each word a number.<br>\n",
        "Count the number of times each word appears in the individual review text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXmpO--gP4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#1. use the words from the training set\n",
        "#2. create a BoW from the yelp reviews\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab = pd.Series(vocab)\n",
        "#2. vectorize the sentences\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "print(\"training data: \", X_train.shape,\"\\ntest data: \", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6MTaQNJXh5",
        "colab_type": "text"
      },
      "source": [
        "What has been done so far: \n",
        "1. Created a vocabulary from all the words used in the yelp reviews.\n",
        "2. Assigned each word a number.<br>\n",
        "\n",
        "Now check the vectorization of the sentences in the yelp review training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GFHSK8qJDAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_train[check])\n",
        "print(X_train[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIn6nF381OLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_test[check])\n",
        "print(X_test[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRTOQFRqMdbX",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #2**: \n",
        "1. Split the Amazon reviews into a training and test set.\n",
        "2. Create a BoW using the amazon training and test reviews. \n",
        "2. Print out the vectorization of two or three reviews from the test and training set. <br>\n",
        "**Make sure your variable names for the Amazon BoW are different than the Yelp BoW.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ8WySaNjHQg",
        "colab_type": "text"
      },
      "source": [
        "# **Create a logistic regression model and train it**<br>\n",
        "\n",
        "The [Logistic Regression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from Scikit Learn implements regularized logistic regression. It can handle both dense and sparse input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXYDqEdrjHar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4tRFkQBjhR1",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 3:**\n",
        "\n",
        "Perform logistic regression on the amazon data set<br>\n",
        "\n",
        "\n",
        "Get a baseline using logistic regression. This will give us something to compare with the other methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74YPIkbjYMu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    reviews = df_source['sentence'].values\n",
        "    reviews_y = df_source['label'].values\n",
        "\n",
        "    reviews_train, reviews_test, reviews_y_train, reviews_y_test = train_test_split(\n",
        "        reviews, reviews_y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(reviews_train)\n",
        "    r_X_train = vectorizer.transform(reviews_train)\n",
        "    r_X_test  = vectorizer.transform(reviews_test)\n",
        "\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(r_X_train, reviews_y_train)\n",
        "    score = classifier.score(r_X_test, reviews_y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}