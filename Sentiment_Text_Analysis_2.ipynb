{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyN7EP5C0iQSZhjNOOxl8CEy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content/\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS"
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf"
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ"
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15"
      },
      "source": [
        "# **Split the review data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences_yelp = df_yelp['sentence'].values\n",
        "y_yelp = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_yelp, sentences_test_yelp, y_train_yelp, y_test_yelp = train_test_split(\n",
        "   sentences_yelp, y_yelp, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_yelp[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnHMFUmoaW17"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_amazon = df[df['source'] == 'amazon']\n",
        "\n",
        "sentences_amazon = df_amazon['sentence'].values\n",
        "y_amazon = df_amazon['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train_amazon, sentences_test_amazon, y_train_amazon, y_test_amazon = train_test_split(\n",
        "   sentences_amazon, y_amazon, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train_amazon[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0D7qx5gPtD"
      },
      "source": [
        "# **Vectorize the training and test set**\n",
        "Vectorize the data: <br>\n",
        "\n",
        "Assign each word a number.<br>\n",
        "Count the number of times each word appears in the individual review text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXmpO--gP4L"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#1. use the words from the training set\n",
        "#2. create a BoW from the yelp reviews\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_yelp)\n",
        "vocab_yelp = vectorizer.vocabulary_\n",
        "vocab_yelp = pd.Series(vocab_yelp)\n",
        "#2. vectorize the sentences\n",
        "X_train_yelp = vectorizer.transform(sentences_train_yelp)\n",
        "X_test_yelp  = vectorizer.transform(sentences_test_yelp)\n",
        "print(\"training data: \", X_train_yelp.shape,\"\\ntest data: \", X_test_yelp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0lHRTnEbKoi"
      },
      "source": [
        "#1. use the words from the training set\n",
        "#2. create a BoW from the yelp reviews\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_amazon)\n",
        "vocab_amazon = vectorizer.vocabulary_\n",
        "vocab_amazon = pd.Series(vocab_amazon)\n",
        "#2. vectorize the sentences\n",
        "X_train_amazon = vectorizer.transform(sentences_train_amazon)\n",
        "X_test_amazon  = vectorizer.transform(sentences_test_amazon)\n",
        "print(\"training data: \", X_train_amazon.shape,\"\\ntest data: \", X_test_amazon.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6MTaQNJXh5"
      },
      "source": [
        "What has been done so far: \n",
        "1. Created a vocabulary from all the words used in the yelp reviews.\n",
        "2. Assigned each word a number.<br>\n",
        "\n",
        "Now check the vectorization of the sentences in the yelp review training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GFHSK8qJDAX"
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_train_yelp[check])\n",
        "print(X_train_yelp[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIn6nF381OLq"
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_test_yelp[check])\n",
        "print(X_test_yelp[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79mkxWjSbnpY"
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=45\n",
        "print(sentences_test_amazon[check])\n",
        "print(X_test_amazon[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWkguVMWlc3b"
      },
      "source": [
        "# **Trial 1:Keras DNN**\n",
        "Create a DNN using Keras, use the Yelp reviews bag of words.<br> \n",
        "Compare it to the logistic regession using the same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRZwkRe4T3Ay"
      },
      "source": [
        "page(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_GA_keldAq"
      },
      "source": [
        "input_dim_yelp = X_train_yelp.shape[1]  # Number of features\n",
        "print(\"model imputs = \", input_dim_yelp)\n",
        "\n",
        "model_yelp = Sequential()\n",
        "model_yelp.add(layers.Dense(1700, input_dim=input_dim_yelp, activation='relu'))\n",
        "model_yelp.add(layers.Dense(1000,  activation='relu'))\n",
        "model_yelp.add(layers.Dense(100,  activation='relu'))\n",
        "model_yelp.add(layers.Dense(1, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2HnnVKQUdVF"
      },
      "source": [
        "# Discussion:\n",
        "Given the dataset and the model, what do you expect to happen? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfbM8Plr_6e"
      },
      "source": [
        "model_yelp.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model_yelp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0lvs-h7CkW"
      },
      "source": [
        "**Train the DNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClsUloHsfeV"
      },
      "source": [
        "history = model_yelp.fit(X_train_yelp, y_train_yelp,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test_yelp, y_test_yelp),\n",
        "                    batch_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNfZaUNtssWj"
      },
      "source": [
        "loss, accuracy = model_yelp.evaluate(X_train_yelp, y_train_yelp, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model_yelp.evaluate(X_test_yelp, y_test_yelp, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy from the Scikit Learn model in notebook 1:<br>\n",
        ">Accuracy for yelp data: 0.7960<br>\n",
        "Accuracy for amazon data: 0.7960<br>\n",
        "Accuracy for imdb data: 0.7487<br>"
      ],
      "metadata": {
        "id": "aDHSIDblbn0T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YFCVmuszj0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAPUYZE-Ddt"
      },
      "source": [
        "The Deep Neural Network trained using Bag of Words is overfit. <br>\n",
        "Play with the model architecture and hyperparameters to see if you can find a better model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNL-Xsls22X"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMV7FP8-Yob"
      },
      "source": [
        "In the BOW model, you  represented an entire review as a single feature vector. In the next section, each word is represented as a vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM3Y-3IsPKNA"
      },
      "source": [
        "# **Assignment #4:**\n",
        "Modify the DNN to see if you can improve the accuracy and loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQy0TojyOkNM"
      },
      "source": [
        "# **Assignment #5:** \n",
        "Train the DNN with the amazon reivews.<br>\n",
        "<br>\n",
        "Use different variable names than the ones used for the Yelp reviews. "
      ]
    }
  ]
}